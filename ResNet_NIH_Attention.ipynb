{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "from typing import Any, List, Tuple\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "from skimage.measure import label\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-12GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('/home/parkar.s/NIH_multilabel_classification/Data_Entry_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
       "0                        0.143  0.143          NaN  \n",
       "1                        0.143  0.143          NaN  \n",
       "2                        0.168  0.168          NaN  \n",
       "3                        0.171  0.171          NaN  \n",
       "4                        0.143  0.143          NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PA    67310\n",
       "AP    44810\n",
       "Name: View Position, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['View Position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 14\n",
    "CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/scratch/parkar.s/NIH/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "targets = []\n",
    "with open('/home/parkar.s/NIH_multilabel_classification/NIH_labels/train_list.txt') as fp:\n",
    "    for line in fp:\n",
    "        file, target = line.split(' ',1)\n",
    "        a = os.path.join(file_path,file)\n",
    "        train_list.append(a)\n",
    "        targets.append([int(i) for i in target.strip().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/scratch/parkar.s/NIH/images/00023313_004.png',\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[5], targets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        image_paths = [],\n",
    "        targets = [],\n",
    "        resize = None,\n",
    "        augmentations=None,\n",
    "        backend=\"cv2\",\n",
    "        channel_first= True\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param image_paths: list of paths to images\n",
    "        :param targets: numpy array\n",
    "        :param resize: tuple or None\n",
    "        :param augmentations: albumentations augmentations\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.resize = resize\n",
    "        self.augmentations = augmentations\n",
    "        self.backend = backend\n",
    "        self.channel_first = channel_first\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.targets = []\n",
    "        with open(data_path) as fp:\n",
    "            for line in fp:\n",
    "                file, target = line.split(' ',1)\n",
    "                a = os.path.join(file_path,file)\n",
    "                self.image_paths.append(a)\n",
    "                self.targets.append([int(i) for i in target.strip().split()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        targets = self.targets[item]\n",
    "        \n",
    "        if self.backend == \"cv2\":\n",
    "            \n",
    "            #Load the image\n",
    "            image = cv2.imread(self.image_paths[item],1)\n",
    "            \n",
    "            #Resize\n",
    "            if self.resize is not None:\n",
    "                image = cv2.resize(\n",
    "                    image,\n",
    "                    (self.resize[1], self.resize[0]),\n",
    "                    interpolation=cv2.INTER_CUBIC,\n",
    "                )\n",
    "            \n",
    "            if self.augmentations is not None:\n",
    "                augmented = self.augmentations(image=image)\n",
    "                image = augmented[\"image\"]\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Backend not implemented\")\n",
    "        \n",
    "        # converting to pytorch image format & 2,0,1 because pytorch excepts image channel first then dimension of image\n",
    "        if self.channel_first:\n",
    "            image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        \n",
    "        # finally returning image tensor and its image id\n",
    "        return {\n",
    "            \"image\": torch.tensor(image),\n",
    "            \"targets\": torch.tensor(targets),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "LR_G = 1e-8\n",
    "LR_L = 1e-8\n",
    "LR_F = 1e-3\n",
    "num_epochs = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "FLAGS = {\n",
    "    'fold': 0,\n",
    "    'model': 'resnet152d',\n",
    "    'pretrained': True,\n",
    "    'batch_size': 4,\n",
    "    'num_workers': 4,\n",
    "    'lr': 3e-4,\n",
    "    'epochs': 10,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'cuda': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = albumentations.Compose(\n",
    "    [       albumentations.Resize(224, 224),\n",
    "            albumentations.CenterCrop(224, 224),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0,\n",
    "            )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/parkar.s/NIH_multilabel_classification/NIH_labels/train_list.txt'\n",
    "\n",
    "train_dataset = ImageDataset(\n",
    "    data_path = train_path,\n",
    "    resize= None,\n",
    "    augmentations= train_aug,\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "The drop_last=True parameter ignores the last batch \n",
    "(when the number of examples in your dataset is not divisible by your batch_size) \n",
    "while drop_last=False will make the last batch smaller than your batch_size \n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=True, num_workers=4, pin_memory=True, drop_last = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4904"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/home/parkar.s/NIH_multilabel_classification/NIH_labels/val_list.txt'\n",
    "\n",
    "valid_dataset = ImageDataset(\n",
    "    data_path = val_path,\n",
    "    resize= None,\n",
    "    augmentations= train_aug,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=True, num_workers=4, pin_memory=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img_dict):\n",
    "    image_tensor = img_dict[\"image\"]\n",
    "    target = img_dict[\"targets\"]\n",
    "    print(target)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    image = image_tensor.permute(1, 2, 0)\n",
    "    \n",
    "    '''\n",
    "    Loading an RGB image will result in an image consisting of integer values\n",
    "    Converting into tensor will convert them into float. Now when you try to display such an image, you ll get the following error-\n",
    "    Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "    \n",
    "    So we first need to convert it back to uint8\n",
    "    '''\n",
    "    \n",
    "    plt.imshow(image.numpy().astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUCs(gt, pred):\n",
    "    \"\"\"Computes Area Under the Curve (AUC) from prediction scores.\n",
    "    Args:\n",
    "        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          true binary labels.\n",
    "        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          can either be probability estimates of the positive class,\n",
    "          confidence values, or binary decisions.\n",
    "    Returns:\n",
    "        List of AUROCs of all classes.\n",
    "    \"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention_gen_patchs(ori_image, fm_cuda):\n",
    "    # feature map -> feature mask (using feature map to crop on the original image) -> crop -> patchs\n",
    "    feature_conv = fm_cuda.data.cpu().numpy()\n",
    "    size_upsample = (224, 224) \n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "\n",
    "    patchs_cuda = torch.FloatTensor().cuda()\n",
    "\n",
    "    for i in range(0, bz):\n",
    "        feature = feature_conv[i]\n",
    "        cam = feature.reshape((nc, h*w))\n",
    "        cam = cam.sum(axis=0)\n",
    "        cam = cam.reshape(h,w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "\n",
    "        heatmap_bin = binImage(cv2.resize(cam_img, size_upsample))\n",
    "        heatmap_maxconn = selectMaxConnect(heatmap_bin)\n",
    "        heatmap_mask = heatmap_bin * heatmap_maxconn\n",
    "\n",
    "        ind = np.argwhere(heatmap_mask != 0)\n",
    "        minh = min(ind[:,0])\n",
    "        minw = min(ind[:,1])\n",
    "        maxh = max(ind[:,0])\n",
    "        maxw = max(ind[:,1])\n",
    "        \n",
    "        # to ori image \n",
    "        image = ori_image[i].numpy().reshape(224,224,3)\n",
    "        image = image[int(224*0.334):int(224*0.667),int(224*0.334):int(224*0.667),:]\n",
    "\n",
    "        image = cv2.resize(image, size_upsample)\n",
    "        image_crop = image[minh:maxh,minw:maxw,:] * 256 # because image was normalized before\n",
    "        image_crop = preprocess(Image.fromarray(image_crop.astype('uint8')).convert('RGB')) \n",
    "\n",
    "        img_variable = torch.autograd.Variable(image_crop.reshape(3,224,224).unsqueeze(0).cuda())\n",
    "\n",
    "        patchs_cuda = torch.cat((patchs_cuda,img_variable),0)\n",
    "\n",
    "    return patchs_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binImage(heatmap):\n",
    "    _, heatmap_bin = cv2.threshold(heatmap , 0 , 255 , cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    # t in the paper\n",
    "    #_, heatmap_bin = cv2.threshold(heatmap , 178 , 255 , cv2.THRESH_BINARY)\n",
    "    return heatmap_bin\n",
    "\n",
    "\n",
    "def selectMaxConnect(heatmap):\n",
    "    labeled_img, num = label(heatmap, connectivity=2, background=0, return_num=True)    \n",
    "    max_label = 0\n",
    "    max_num = 0\n",
    "    for i in range(1, num+1):\n",
    "        if np.sum(labeled_img == i) > max_num:\n",
    "            max_num = np.sum(labeled_img == i)\n",
    "            max_label = i\n",
    "    lcc = (labeled_img == max_label)\n",
    "    if max_num == 0:\n",
    "        lcc = (labeled_img == -1)\n",
    "    lcc = lcc + 0\n",
    "    return lcc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((256,256)),\n",
    "   transforms.CenterCrop(224),\n",
    "   transforms.ToTensor(),\n",
    "   normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Only Resnet (Pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #device, will be different for each core on the TPU\n",
    "epochs = FLAGS['epochs']\n",
    "fold = FLAGS['fold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(data_loader, loss_fn, model, optimizer, device, scheduler, epoch):\n",
    "    model.train() # put model in training mode\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    for index, d in enumerate(data_loader): # enumerate through the dataloader\n",
    "        \n",
    "        images = d['image'] # obtain the ids\n",
    "        targets = d['targets'] # obtain the target\n",
    "        if FLAGS['cuda']:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # pass image to model\n",
    "        \n",
    "        # clear out the accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make predictions\n",
    "        outputs = model(images)\n",
    "        \n",
    "        targets = targets.to(torch.float32)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        if (index%500) == 0: \n",
    "            print('step: {} totalloss: {loss:.3f} '.format(index, loss = loss))\n",
    "\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Performs parameter update\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "            \n",
    "    epoch_loss = float(running_loss) / float(index)\n",
    "    print(' Epoch over  Loss: {:.5f}'.format(epoch_loss))\n",
    "\n",
    "     # put model in eval mode for later use\n",
    "    \n",
    "def eval_loop_fn(data_loader, loss_fn, model, device):\n",
    "    \n",
    "    #will notify all your layers that you are in eval mode,\n",
    "    #that way, batchnorm or dropout layers will work in eval\n",
    "    #mode instead of training mode.\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    for bi, d in enumerate(data_loader): # enumerate through dataloader\n",
    "        \n",
    "        images = d['image'] # obtain the ids\n",
    "        targets = d['targets']# # obtain the targets\n",
    "        \n",
    "        if FLAGS['cuda']:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "\n",
    "        # pass image to model\n",
    "        \n",
    "        # no_grad impacts the autograd engine and deactivate it.\n",
    "        # It will reduce memory usage and speed up computations\n",
    "        # but you won’t be able to backprop\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(images)\n",
    "\n",
    "        # Add the outputs and targets to a list \n",
    "        targets_np = targets.cpu().detach().numpy().tolist()\n",
    "        outputs_np = outputs.cpu().detach().numpy().tolist()\n",
    "        fin_targets.extend(targets_np)\n",
    "        fin_outputs.extend(outputs_np)    \n",
    "        del targets_np, outputs_np\n",
    "        gc.collect() # delete for memory conservation\n",
    "                \n",
    "    o,t = np.array(fin_outputs), np.array(fin_targets)\n",
    "    \n",
    "\n",
    "    \n",
    "    # calculate loss\n",
    "    # loss = loss_fn(torch.tensor(o), t)\n",
    "    \n",
    "    AUROCs_g = compute_AUCs(torch.tensor(t), torch.tensor(o))\n",
    "    AUROC_avg = np.array(AUROCs_g).mean()\n",
    "    print('Global branch: The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(N_CLASSES):\n",
    "        print('The AUROC of {} is {}'.format(CLASS_NAMES[i], AUROCs_g[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ross Wightman's timm package\n",
    "class TimmModels(nn.Module):\n",
    "    def __init__(self, model_name,pretrained=True, num_classes=3):\n",
    "        super(TimmModels, self).__init__()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "\n",
    "        self.m = timm.create_model(model_name,pretrained=pretrained)\n",
    "        model_list = list(self.m.children())\n",
    "        model_list = model_list[:-2]\n",
    "        model_list[-1][-1].act3 = nn.Identity()\n",
    "        self.m = nn.Sequential(*model_list)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        features = self.m(image)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out_after_pooling = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifier(out_after_pooling)\n",
    "        out = self.Sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion_Branch(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Fusion_Branch, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, global_pool, local_pool):\n",
    "        #fusion = torch.cat((global_pool.unsqueeze(2), local_pool.unsqueeze(2)), 2).cuda()\n",
    "        #fusion = fusion.max(2)[0]#.squeeze(2).cuda()\n",
    "        #print(fusion.shape)\n",
    "        fusion = torch.cat((global_pool,local_pool), 1).cuda()\n",
    "        fusion_var = torch.autograd.Variable(fusion)\n",
    "        x = self.fc(fusion_var)\n",
    "        x = self.Sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MX = TimmModels(FLAGS['model'],pretrained=FLAGS['pretrained'], num_classes=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MX.to(device) # put model onto the current GPU\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=FLAGS['lr']) # often a good idea to scale the learning rate by number of cores\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*FLAGS['epochs']) #let's use a scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f'========== training fold {FLAGS[\"fold\"]} for {FLAGS[\"epochs\"]} epochs ==========')\n",
    "for i in range(2):\n",
    "    print(f'EPOCH {i}:')\n",
    "    # train one epoch\n",
    "    train_loop_fn(train_loader, loss_fn, model, optimizer, device, scheduler, i)\n",
    "\n",
    "    # validation one epoch\n",
    "    eval_loop_fn(valid_loader, loss_fn, model, device)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print('Saving model...')\n",
    "\n",
    "torch.save(model.state_dict(), f'resnet_NIH_{FLAGS[\"epochs\"]}_epochs_pretrained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/home/parkar.s/NIH_multilabel_classification/NIH_labels/test_list.txt'\n",
    "\n",
    "test_dataset = ImageDataset(\n",
    "    data_path = val_path,\n",
    "    resize= None,\n",
    "    augmentations= train_aug,\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=True, num_workers=4, pin_memory=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MX_test = TimmModels(FLAGS['model'],pretrained=FLAGS['pretrained'], num_classes=14)\n",
    "model_test = MX_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'resnet_NIH_10_epochs_pretrained.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global branch: The average AUROC is 0.831\n",
      "The AUROC of Atelectasis is 0.8146734892948528\n",
      "The AUROC of Cardiomegaly is 0.907767266665094\n",
      "The AUROC of Effusion is 0.8788024915771728\n",
      "The AUROC of Infiltration is 0.7049163282638219\n",
      "The AUROC of Mass is 0.8338879825915774\n",
      "The AUROC of Nodule is 0.768601744856423\n",
      "The AUROC of Pneumonia is 0.7596380247226246\n",
      "The AUROC of Pneumothorax is 0.863845226467845\n",
      "The AUROC of Consolidation is 0.7939528478960858\n",
      "The AUROC of Edema is 0.8908690128611136\n",
      "The AUROC of Emphysema is 0.9247012695891238\n",
      "The AUROC of Fibrosis is 0.8347302780955623\n",
      "The AUROC of Pleural_Thickening is 0.7795339818358678\n",
      "The AUROC of Hernia is 0.8815617090963228\n"
     ]
    }
   ],
   "source": [
    "eval_loop_fn(test_loader, loss_fn, model_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #device, will be different for each core on the TPU\n",
    "epochs = FLAGS['epochs']\n",
    "fold = FLAGS['fold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = {\n",
    "    'fold': 0,\n",
    "    'model': 'resnet152d',\n",
    "    'pretrained': True,\n",
    "    'batch_size': 4,\n",
    "    'num_workers': 4,\n",
    "    'lr': 3e-6,\n",
    "    'lr_f': 1e-4,\n",
    "    'epochs': 10,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'cuda': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ross Wightman's timm package\n",
    "class TimmModels(nn.Module):\n",
    "    def __init__(self, model_name,pretrained=True, num_classes=3):\n",
    "        super(TimmModels, self).__init__()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "\n",
    "        self.m = timm.create_model(model_name,pretrained=pretrained)\n",
    "        model_list = list(self.m.children())\n",
    "        model_list = model_list[:-2]\n",
    "        model_list[-1][-1].act3 = nn.Identity()\n",
    "        self.m = nn.Sequential(*model_list)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        features = self.m(image)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out_after_pooling = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifier(out_after_pooling)\n",
    "        out = self.Sigmoid(out)\n",
    "        return out, features, out_after_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion_Branch(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Fusion_Branch, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, global_pool, local_pool):\n",
    "        #fusion = torch.cat((global_pool.unsqueeze(2), local_pool.unsqueeze(2)), 2).cuda()\n",
    "        #fusion = fusion.max(2)[0]#.squeeze(2).cuda()\n",
    "        #print(fusion.shape)\n",
    "        fusion = torch.cat((global_pool,local_pool), 1).cuda()\n",
    "        fusion_var = torch.autograd.Variable(fusion)\n",
    "        x = self.fc(fusion_var)\n",
    "        x = self.Sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_attn(data_loader, loss_fn, Global_Branch_model,Local_Branch_model, Fusion_Branch_model, \n",
    "                    optimizer_global, optimizer_local, optimizer_fusion, \n",
    "                    lr_scheduler_global, lr_scheduler_local, lr_scheduler_fusion, device, epoch):\n",
    "    # put model in training mode\n",
    "    Global_Branch_model.train()  #set model to training mode\n",
    "    Local_Branch_model.train()\n",
    "    Fusion_Branch_model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    for index, d in enumerate(data_loader): # enumerate through the dataloader\n",
    "        \n",
    "        images = d['image'] # obtain the ids\n",
    "        targets = d['targets'] # obtain the target\n",
    "        if FLAGS['cuda']:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # pass image to model\n",
    "        \n",
    "        # clear out the accumulated gradients\n",
    "        \n",
    "        optimizer_global.zero_grad()\n",
    "        optimizer_local.zero_grad()\n",
    "        optimizer_fusion.zero_grad()\n",
    "        \n",
    "        # make predictions\n",
    "\n",
    "        output_global, fm_global, pool_global = Global_Branch_model(images)\n",
    "        \n",
    "        patchs_var = Attention_gen_patchs(images.cpu(),fm_global)\n",
    "        \n",
    "        output_local, _, pool_local = Local_Branch_model(patchs_var)\n",
    "\n",
    "        output_fusion = Fusion_Branch_model(pool_global, pool_local)\n",
    "        \n",
    "        targets = targets.to(torch.float32)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss1 = loss_fn(output_global, targets)\n",
    "        loss2 = loss_fn(output_local, targets)\n",
    "        loss3 = loss_fn(output_fusion, targets)\n",
    "        \n",
    "        loss = loss1*0.8 + loss2*0.1 + loss3*0.1 \n",
    "\n",
    "        if (index%500) == 0: \n",
    "            print('step: {} totalloss: {loss:.3f} loss1: {loss1:.3f} loss2: {loss2:.3f} loss3: {loss3:.3f}'.format(index, loss = loss, loss1 = loss1, loss2 = loss2, loss3 = loss3))\n",
    "\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Performs parameter update\n",
    "        optimizer_global.step()  \n",
    "        optimizer_local.step()\n",
    "        optimizer_fusion.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "        # Step the scheduler\n",
    "        lr_scheduler_global.step()\n",
    "        lr_scheduler_local.step()\n",
    "        lr_scheduler_fusion.step()\n",
    "            \n",
    "    epoch_loss = float(running_loss) / float(index)\n",
    "    print(' Epoch over  Loss: {:.5f}'.format(epoch_loss))\n",
    "\n",
    "    \n",
    "     # put model in eval mode for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_attn(data_loader, loss_fn, model_global, model_local, model_fusion, device):\n",
    "    \n",
    "    #will notify all your layers that you are in eval mode,\n",
    "    #that way, batchnorm or dropout layers will work in eval\n",
    "    #mode instead of training mode.\n",
    "    \n",
    "    model_global.eval()\n",
    "    model_local.eval()\n",
    "    model_fusion.eval()\n",
    "    \n",
    "    fin_targets = []\n",
    "    global_outputs = []\n",
    "    local_outputs = []\n",
    "    fusion_outputs = []\n",
    "    \n",
    "    for bi, d in enumerate(data_loader): # enumerate through dataloader\n",
    "        \n",
    "        images = d['image'] # obtain the ids\n",
    "        targets = d['targets']# # obtain the targets\n",
    "        \n",
    "        if FLAGS['cuda']:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "\n",
    "        # pass image to model\n",
    "        \n",
    "        # no_grad impacts the autograd engine and deactivate it.\n",
    "        # It will reduce memory usage and speed up computations\n",
    "        # but you won’t be able to backprop\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            output_global, fm_global, pool_global = model_global(images)\n",
    "        \n",
    "            patchs_var = Attention_gen_patchs(images.cpu(),fm_global)\n",
    "\n",
    "            output_local, _, pool_local = model_local(patchs_var)\n",
    "\n",
    "            output_fusion = model_fusion(pool_global, pool_local)\n",
    "\n",
    "        # Add the outputs and targets to a list \n",
    "        targets_np = targets.cpu().detach().numpy().tolist()\n",
    "        outputs_g = output_global.cpu().detach().numpy().tolist()\n",
    "        outputs_l = output_local.cpu().detach().numpy().tolist()\n",
    "        outputs_f = output_fusion.cpu().detach().numpy().tolist()\n",
    "        \n",
    "        \n",
    "        fin_targets.extend(targets_np) \n",
    "        \n",
    "        global_outputs.extend(outputs_g)\n",
    "        local_outputs.extend(outputs_l)\n",
    "        fusion_outputs.extend(outputs_f)\n",
    "        \n",
    "        del targets_np, outputs_g, outputs_l, outputs_f\n",
    "        gc.collect() # delete for memory conservation\n",
    "                \n",
    "    og, ol, of, t = np.array(global_outputs), np.array(local_outputs), np.array(fusion_outputs), np.array(fin_targets)\n",
    "    \n",
    "\n",
    "    \n",
    "    # calculate loss\n",
    "    # loss = loss_fn(torch.tensor(o), t)\n",
    "    \n",
    "    AUROCs_g = compute_AUCs(torch.tensor(t), torch.tensor(og))\n",
    "    AUROC_avg = np.array(AUROCs_g).mean()\n",
    "    print('Global branch: The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(N_CLASSES):\n",
    "        print('The AUROC of {} is {}'.format(CLASS_NAMES[i], AUROCs_g[i]))\n",
    "    \n",
    "    AUROCs_l = compute_AUCs(torch.tensor(t), torch.tensor(ol))\n",
    "    AUROC_avg = np.array(AUROCs_l).mean()\n",
    "    print('\\n')\n",
    "    print('Local branch: The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(N_CLASSES):\n",
    "        print('The AUROC of {} is {}'.format(CLASS_NAMES[i], AUROCs_l[i]))\n",
    "\n",
    "    AUROCs_f = compute_AUCs(torch.tensor(t), torch.tensor(of))\n",
    "    AUROC_avg = np.array(AUROCs_f).mean()\n",
    "    print('\\n')\n",
    "    print('Fusion branch: The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(N_CLASSES):\n",
    "        print('The AUROC of {} is {}'.format(CLASS_NAMES[i], AUROCs_f[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== training fold 0 for 10 epochs ==========\n",
      "EPOCH 0:\n",
      "step: 0 totalloss: 0.695 loss1: 0.693 loss2: 0.699 loss3: 0.702\n",
      "step: 500 totalloss: 0.390 loss1: 0.395 loss2: 0.440 loss3: 0.293\n",
      "step: 1000 totalloss: 0.190 loss1: 0.185 loss2: 0.232 loss3: 0.191\n",
      "step: 1500 totalloss: 0.202 loss1: 0.201 loss2: 0.202 loss3: 0.210\n",
      "step: 2000 totalloss: 0.145 loss1: 0.144 loss2: 0.137 loss3: 0.159\n",
      "step: 2500 totalloss: 0.183 loss1: 0.180 loss2: 0.184 loss3: 0.205\n",
      "step: 3000 totalloss: 0.149 loss1: 0.143 loss2: 0.167 loss3: 0.177\n",
      "step: 3500 totalloss: 0.090 loss1: 0.087 loss2: 0.091 loss3: 0.106\n",
      "step: 4000 totalloss: 0.127 loss1: 0.126 loss2: 0.123 loss3: 0.135\n",
      "step: 4500 totalloss: 0.127 loss1: 0.120 loss2: 0.142 loss3: 0.170\n",
      " Epoch over  Loss: 0.22480\n",
      "Global branch: The average AUROC is 0.691\n",
      "The AUROC of Atelectasis is 0.6991853728396662\n",
      "The AUROC of Cardiomegaly is 0.6433295483176501\n",
      "The AUROC of Effusion is 0.7787464919144008\n",
      "The AUROC of Infiltration is 0.6418663858282632\n",
      "The AUROC of Mass is 0.6565093381172694\n",
      "The AUROC of Nodule is 0.6291761742459852\n",
      "The AUROC of Pneumonia is 0.6371228983765016\n",
      "The AUROC of Pneumothorax is 0.7633084872921039\n",
      "The AUROC of Consolidation is 0.7631230001269284\n",
      "The AUROC of Edema is 0.7816929920116196\n",
      "The AUROC of Emphysema is 0.6697410624440965\n",
      "The AUROC of Fibrosis is 0.6416414981191735\n",
      "The AUROC of Pleural_Thickening is 0.7155374063453156\n",
      "The AUROC of Hernia is 0.6582986850002728\n",
      "\n",
      "\n",
      "Local branch: The average AUROC is 0.598\n",
      "The AUROC of Atelectasis is 0.592837943795054\n",
      "The AUROC of Cardiomegaly is 0.5864472043675956\n",
      "The AUROC of Effusion is 0.6530295493428072\n",
      "The AUROC of Infiltration is 0.5950169123679449\n",
      "The AUROC of Mass is 0.5723724671891228\n",
      "The AUROC of Nodule is 0.5360188465851718\n",
      "The AUROC of Pneumonia is 0.6037330084210797\n",
      "The AUROC of Pneumothorax is 0.6010381288749007\n",
      "The AUROC of Consolidation is 0.6826645086785896\n",
      "The AUROC of Edema is 0.7171750181554104\n",
      "The AUROC of Emphysema is 0.5401161480461762\n",
      "The AUROC of Fibrosis is 0.5331477947991059\n",
      "The AUROC of Pleural_Thickening is 0.5668116355905649\n",
      "The AUROC of Hernia is 0.590618213564686\n",
      "\n",
      "\n",
      "Fusion branch: The average AUROC is 0.648\n",
      "The AUROC of Atelectasis is 0.6516815486740193\n",
      "The AUROC of Cardiomegaly is 0.6120526639382535\n",
      "The AUROC of Effusion is 0.7276705048149215\n",
      "The AUROC of Infiltration is 0.6179686420821004\n",
      "The AUROC of Mass is 0.6138003210272874\n",
      "The AUROC of Nodule is 0.5624557763900426\n",
      "The AUROC of Pneumonia is 0.6453784465675603\n",
      "The AUROC of Pneumothorax is 0.6952879577272781\n",
      "The AUROC of Consolidation is 0.7167713357360375\n",
      "The AUROC of Edema is 0.7269725853304285\n",
      "The AUROC of Emphysema is 0.627720049754025\n",
      "The AUROC of Fibrosis is 0.5655160006542005\n",
      "The AUROC of Pleural_Thickening is 0.6759203345192624\n",
      "The AUROC of Hernia is 0.6326512795329295\n",
      "EPOCH 1:\n",
      "step: 0 totalloss: 0.179 loss1: 0.173 loss2: 0.185 loss3: 0.222\n",
      "step: 500 totalloss: 0.166 loss1: 0.161 loss2: 0.176 loss3: 0.189\n",
      "step: 1000 totalloss: 0.165 loss1: 0.161 loss2: 0.171 loss3: 0.192\n",
      "step: 1500 totalloss: 0.189 loss1: 0.187 loss2: 0.200 loss3: 0.196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e9d45d8e8262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'EPOCH {i}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# train one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_with_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mGlobal_Branch_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLocal_Branch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFusion_Branch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_fusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler_fusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# validation one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-77aff81c7a3b>\u001b[0m in \u001b[0;36mtrain_with_attn\u001b[0;34m(data_loader, loss_fn, Global_Branch_model, Local_Branch_model, Fusion_Branch_model, optimizer_global, optimizer_local, optimizer_fusion, lr_scheduler_global, lr_scheduler_local, lr_scheduler_fusion, device, epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfm_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobal_Branch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpatchs_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention_gen_patchs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfm_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-5cf19c319d4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mout_after_pooling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/timm/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_block\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MX_global = TimmModels(FLAGS['model'],pretrained=FLAGS['pretrained'], num_classes=14)\n",
    "MX_local = TimmModels(FLAGS['model'],pretrained=FLAGS['pretrained'], num_classes=14)\n",
    "\n",
    "Global_Branch_model = MX_global.to(device) # put model onto the current GPU\n",
    "Local_Branch_model = MX_local.to(device) # put model onto the current GPU\n",
    "Fusion_Branch_model = Fusion_Branch(input_size = 4096, output_size = N_CLASSES).to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "optimizer_global = Adam(Global_Branch_model.parameters(), lr=FLAGS['lr'])\n",
    "optimizer_local = Adam(Local_Branch_model.parameters(), lr=FLAGS['lr'])\n",
    "optimizer_fusion = Adam(Fusion_Branch_model.parameters(), lr=FLAGS['lr_f'])\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*FLAGS['epochs']) #let's use a scheduler\n",
    "\n",
    "lr_scheduler_global = lr_scheduler.StepLR(optimizer_global , step_size = 10, gamma = 1)\n",
    "lr_scheduler_local = lr_scheduler.StepLR(optimizer_local , step_size = 10, gamma = 1)\n",
    "lr_scheduler_fusion = lr_scheduler.StepLR(optimizer_fusion , step_size = 15, gamma = 0.1)\n",
    "\n",
    "print(f'========== training fold {FLAGS[\"fold\"]} for {FLAGS[\"epochs\"]} epochs ==========')\n",
    "for i in range(FLAGS['epochs']):\n",
    "    print(f'EPOCH {i}:')\n",
    "    # train one epoch\n",
    "    train_with_attn(train_loader, loss_fn,  Global_Branch_model,Local_Branch_model, Fusion_Branch_model, optimizer_global, optimizer_local, optimizer_fusion, lr_scheduler_global, lr_scheduler_local, lr_scheduler_fusion, device,   i)\n",
    "\n",
    "    # validation one epoch\n",
    "    eval_with_attn(valid_loader, loss_fn, Global_Branch_model, Local_Branch_model, Fusion_Branch_model, device)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    torch.save(Global_Branch_model.state_dict(), f'densenet_attn_NIH_{FLAGS[\"epochs\"]}_'+str(i)+'_global.pth')\n",
    "    torch.save(Local_Branch_model.state_dict(), f'densenet_attn_NIH_{FLAGS[\"epochs\"]}_'+str(i)+'_local.pth')\n",
    "    torch.save(Fusion_Branch_model.state_dict(), f'densenet_attn_NIH_{FLAGS[\"epochs\"]}_'+str(i)+'_fusion.pth')\n",
    "\n",
    "print('Saving model...')\n",
    "\n",
    "torch.save(Global_Branch_model.state_dict(), f'densenet_attn_NIH_{FLAGS[\"epochs\"]}_global.pth')\n",
    "torch.save(Local_Branch_model.state_dict(), f'densenet_attn_NIH_{FLAGS[\"epochs\"]}_local.pth')\n",
    "torch.save(Fusion_Branch_model.state_dict(), f'densenet_attn_NIH_{FLAGS[\"epochs\"]}_fusion.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
